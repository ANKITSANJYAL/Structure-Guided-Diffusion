# Training Configuration for GeoDreamer

# Data Configuration
data:
  dataset_name: "oxford_flowers_102"
  image_size: 224
  batch_size: 16
  num_workers: 4
  train_split: 0.8
  val_split: 0.2
  
# Model Configuration
model:
  # Diffusion parameters
  diffusion_steps: 1000
  beta_start: 0.0001
  beta_end: 0.02
  schedule: "linear"
  
  # U-Net parameters
  unet:
    in_channels: 3
    out_channels: 3
    model_channels: 128
    num_res_blocks: 2
    attention_resolutions: [8, 16]
    dropout: 0.1
    channel_mult: [1, 2, 4, 8]
    conv_resample: true
    num_heads: 8
    use_spatial_transformer: true
    transformer_depth: 1
    context_dim: 768  # CLIP embedding dimension
    geometry_dim: 768  # DINO embedding dimension
    
  # Geometry encoder
  geometry_encoder:
    model_name: "dinov2_vitb14"
    patch_size: 14
    embed_dim: 768
    num_heads: 12
    num_layers: 12
    
  # Semantic conditioning
  semantic_conditioning:
    model_name: "openai/clip-vit-base-patch32"
    embed_dim: 512

# Training Configuration
training:
  epochs: 100
  learning_rate: 1e-4
  weight_decay: 1e-6
  warmup_steps: 1000
  gradient_clip_val: 1.0
  
  # Loss weights
  loss_weights:
    denoising: 1.0
    geometry_consistency: 0.1
    semantic_alignment: 0.05
    
  # Optimization
  optimizer: "adamw"
  scheduler: "cosine"
  
  # Checkpointing
  save_every_n_epochs: 10
  save_best_model: true
  max_checkpoints: 5
  
  # Logging
  log_every_n_steps: 100
  log_images_every_n_epochs: 5
  use_wandb: true
  wandb_project: "geodreamer"
  
# Evaluation Configuration
evaluation:
  metrics:
    - "fid"
    - "clip_score"
    - "lpips"
    - "ssim"
  eval_every_n_epochs: 5
  num_eval_samples: 1000
  
# Sampling Configuration
sampling:
  num_inference_steps: 50
  guidance_scale: 7.5
  classifier_free_guidance: true
  geometry_guidance_scale: 1.0
  
# Hardware Configuration
hardware:
  device: "auto"  # auto, cuda, cpu
  mixed_precision: true
  num_gpus: 1
  gradient_accumulation_steps: 1 